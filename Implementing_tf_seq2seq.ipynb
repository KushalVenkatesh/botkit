{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing tf-seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/KushalVenkatesh/botkit/blob/master/Implementing_tf_seq2seq.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "iFEvcjKyeXP6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparing The Training Data:\n",
        "\n",
        "Let us start with preparing the data.\n",
        "\n",
        "I will be doing all this in the Google Colab.\n",
        "\n",
        "I will be using NLTK lib that requires some “pre-warming”:"
      ]
    },
    {
      "metadata": {
        "id": "F46asZwfRY7M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b7ce2386-4166-4ab6-8dd7-8fafa861dd57"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "dwlr = nltk.downloader.Downloader()\n",
        "\n",
        "for pkg in dwlr.packages():\n",
        "    if pkg.subdir== 'tokenizers':\n",
        "        dwlr.download(pkg.id)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SzvHiIGheDwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let’s create brand new notebook with Python3 support.\n",
        "\n",
        "As soon as notebook is created I had to change the runtime type and set it to GPU"
      ]
    },
    {
      "metadata": {
        "id": "5Y4ae7RsdFmR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here I will be using the following to demonstrate  neural machine translation (NMT)."
      ]
    },
    {
      "metadata": {
        "id": "spMvbv5USXC-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf dialog_converter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wg2lmUWldi8C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this point one may ask, okay I see what this is all about, but how is it possible to train a model for free? Google recently have announced that they are giving one Nvidia K80 GPU for 12 hours for free with their new service Colab. Essentially Colab is a custom version of Jupyter Notebook.\n",
        "\n",
        "So looks like I have both components:\n",
        "\n",
        "Model that I want to train and\n",
        "service with the Nvidia K80 that I will be using for the actual training.\n",
        "Here I begin the journey…"
      ]
    },
    {
      "metadata": {
        "id": "2BKWjr7Aewms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now I can clone the branch and run the logic that prepares training data:"
      ]
    },
    {
      "metadata": {
        "id": "T2NKNCrkSjNI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "e6324760-5e28-463e-9c84-76c67f444a31"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/b0noI/dialog_converter.git"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dialog_converter'...\n",
            "remote: Counting objects: 140, done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 140 (delta 6), reused 16 (delta 5), pack-reused 122\u001b[K\n",
            "Receiving objects: 100% (140/140), 15.13 MiB | 28.01 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3jyjpEHaSpgN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd dialog_converter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zqzRZFZLTILu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "004cbf23-a7a5-49a1-d1bf-1548047c3e49"
      },
      "cell_type": "code",
      "source": [
        "!git checkout b9cc7b7d82a959c80e5048b18e956841233c7688"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: Not a git repository (or any of the parent directories): .git\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HGs4C1LrTOHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d738d248-6a2e-41ce-f5b1-ef03063d7d40"
      },
      "cell_type": "code",
      "source": [
        "!python ./converter.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file './converter.py': [Errno 2] No such file or directory\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vgOWFZIMTT3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c68b0084-6154-4021-c5f7-0ba504cf8dd1"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab  dialog_converter  input  input.pre  input.pre.bpe  nltk_data  output\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tuDoryHcTZAc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd dialog_converter/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l95CNFT1TfUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc2508fe-a48f-425e-e258-4dc181747174"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab  dialog_converter  input  input.pre  input.pre.bpe  nltk_data  output\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d7c55aYPTh6G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd dialog_converter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcEFIsCNTm1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ceb396e-d43b-4bdf-f034-1daa9bf45cc1"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab  dialog_converter  input  input.pre  input.pre.bpe  nltk_data  output\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nDtVPoQAT5wu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dce8541-5e3c-4009-d61d-ec0879de6219"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cjRETFVXUT2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ccc3ec2-29a2-41b7-f7b2-499ed275c903"
      },
      "cell_type": "code",
      "source": [
        "%%bash"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: %%bash is a cell magic, but the cell body is empty.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fyUsxOCYUjfn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "11a6cd93-5731-4a6e-a1b9-e547984dfa5c"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf dialog_converter\n",
        "git clone https://github.com/b0noI/dialog_converter.git\n",
        "cd dialog_converter\n",
        "git checkout b9cc7b7d82a959c80e5048b18e956841233c7688\n",
        "python ./converter.py\n",
        "ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "converter.py\n",
            "LICENSE\n",
            "movie_lines.txt\n",
            "pre_processing.py\n",
            "README.md\n",
            "test.a\n",
            "test.b\n",
            "train.a\n",
            "train.b\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dialog_converter'...\n",
            "Note: checking out 'b9cc7b7d82a959c80e5048b18e956841233c7688'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at b9cc7b7... Updated preprocessing\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZntqIVxlfcKD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now I have raw dialog data for training. One of the key differences this time around is that I are going to have more sophisticated dictionary. In order to explain the difference in our new dictionary:\n",
        "\n",
        "Learning a model based on words has a couple of drawbacks. Because NMT models output a probability distribution over words, they can became very slow with large number of possible words. If I include misspellings and derived words in our vocabulary, the number of possible words is essentially infinite and I need to impose an artificial limit of how of the most common words I want our model to handle. This is also called the vocabulary size and typically set to something in the range of 10,000 to 100,000. Another drawback of training on word tokens is that the model does not learn about common “stems” of words. For example, it would consider “loved” and “loving” as completely separate classes despite their common root.\n",
        "One way to handle an open vocabulary issue is learn subword units for a given text. For example, the word “loved” may be split up into “lov” and “ed”, while “loving” would be split up into “lov” and “ing”. This allows to model to generalize to new words, while also resulting in a smaller vocabulary size. There are several techniques for learning such subword units, including Byte Pair Encoding (BPE), which is what I used here. To generate a BPE for a given text, I can follow the instructions in the official subword-nmt repository:\n",
        "\n",
        "So our next logical steps are:\n",
        "\n",
        "Get all the required software that can learn a BPE vocabulary from training text\n",
        "Convert training data to BPE and create a vocabulary\n",
        "Convert all text with the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "b4RcpgLBgSp8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Inorder to get all the required software that can learn a BPE vocabulary from training text,\n",
        "I will be installing the subword-nmt pip package in order to perform required manipulations. \n",
        "In order to do so I run the following command in the cell:"
      ]
    },
    {
      "metadata": {
        "id": "QGsc7jU9U3mO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e72ef884-e5db-4f66-ea7c-453361d006d7"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf subword-nmt\n",
        "git clone https://github.com/b0noI/subword-nmt.git\n",
        "cd subword-nmt\n",
        "git checkout dbe97c8f95f14d06b2e46b8053e2e2f9b9bf804e"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'subword-nmt'...\n",
            "Note: checking out 'dbe97c8f95f14d06b2e46b8053e2e2f9b9bf804e'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at dbe97c8... bug2 output removed\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gb4mDzd2gzJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Converting Training Data To BPE and Create a Vocabulary\n",
        "\n",
        "I will need to execute this in following 3 steps.\n",
        "\n",
        "Step 1: This step is responsible to create a vocabulary based on input training data and specified size of vocabulary. It creates code.bpe which is basically ‘compressed trie’ of all the words in training data. It also generates most frequent words from training data along with their frequencies in the files vocab.train.bpe.{a,b}."
      ]
    },
    {
      "metadata": {
        "id": "r-K6JrbPVK5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7b595db-2bd2-4ea5-8a2c-62ab3436be8f"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Create unique words (vocabulary) from training data\n",
        "subword-nmt/learn_joint_bpe_and_vocab.py --input dialog_converter/train.a dialog_converter/train.b -s 50000 -o code.bpe --write-vocabulary vocab.train.bpe.a vocab.train.bpe.b"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no pair has frequency >= 2. Stopping\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QVH2fLcGhkDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 2: Our training data has few tabs which are not needed in vocabulary, so lets clean the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "OFHXPOrfVwZG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Remove the tab from vocabulary \n",
        "sed -i '/\\t/d' ./vocab.train.bpe.a\n",
        "sed -i '/\\t/d' ./vocab.train.bpe.b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypiUs65mhuQ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 3: The output files vocab.train.{a,b} has list of words along with their frequencies, tf-seq2seq takes input as set of words, so I can get rid of frequency."
      ]
    },
    {
      "metadata": {
        "id": "LfOeHvpCV6Ys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Remove the frequency from vocabulary\n",
        "cat vocab.train.bpe.a | cut -f1 --delimiter=' ' > revocab.train.bpe.a\n",
        "cat vocab.train.bpe.b | cut -f1 --delimiter=' ' > revocab.train.bpe.b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RoBpbo9Bh5Q7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Converting all text with the vocabulary:\n",
        "\n",
        "This cell will create BPE encoding and dictionaries per each raw file. And now I can re-apply this dictionaries to our raw files:"
      ]
    },
    {
      "metadata": {
        "id": "m9K6XegpV-M9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.a --vocabulary-threshold 5 < dialog_converter/train.a > train.bpe.a\n",
        "subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.b --vocabulary-threshold 5 < dialog_converter/train.b > train.bpe.b\n",
        "subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.a --vocabulary-threshold 5 < dialog_converter/test.a > test.bpe.a\n",
        "subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.b --vocabulary-threshold 5 < dialog_converter/test.b > test.bpe.b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XneJ2pYdiNX7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparation for training:\n",
        "\n",
        "Step 1: Downloading the nmt model"
      ]
    },
    {
      "metadata": {
        "id": "Zc2jwYGrWGqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a8427f5-fead-4541-d943-2a7f4b50fe1e"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf /content/nmt_model\n",
        "rm -rf nmt\n",
        "git clone https://github.com/tensorflow/nmt/"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nmt'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4sSDkarQieEa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 2: Moving all the required file for the training to one place. Which includes the training data, test data and vocabulary, this includes just setting of words."
      ]
    },
    {
      "metadata": {
        "id": "QJIOyCfXi4tR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Starting the training:"
      ]
    },
    {
      "metadata": {
        "id": "spB6u_jHW8Ni",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p /content/nmt_model\n",
        "cp dialog_converter/train.a /content/nmt_model\n",
        "cp dialog_converter/train.b /content/nmt_model\n",
        "cp dialog_converter/test.a /content/nmt_model\n",
        "cp dialog_converter/test.b /content/nmt_model\n",
        "cp revocab.train.bpe.a /content/nmt_model\n",
        "cp revocab.train.bpe.b /content/nmt_model\n",
        "cp train.bpe.a /content/nmt_model\n",
        "cp test.bpe.a /content/nmt_model\n",
        "cp train.bpe.b /content/nmt_model\n",
        "cp test.bpe.b /content/nmt_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKO59QQUXLl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3027
        },
        "outputId": "ccd55eb5-b82a-4825-aafc-cc754bb34481"
      },
      "cell_type": "code",
      "source": [
        "!cd nmt && python3 -m nmt.nmt \\\n",
        "    --src=a --tgt=b \\\n",
        "    --vocab_prefix=/content/nmt_model/revocab.train.bpe \\\n",
        "    --train_prefix=/content/nmt_model/train.bpe \\\n",
        "    --dev_prefix=/content/nmt_model/test.bpe \\\n",
        "    --test_prefix=/content/nmt_model/test.bpe \\\n",
        "    --out_dir=/content/nmt_model \\\n",
        "    --num_train_steps=45000000 \\\n",
        "    --steps_per_stats=100000 \\\n",
        "    --num_layers=2 \\\n",
        "    --num_units=128 \\\n",
        "    --batch_size=16 \\\n",
        "    --num_gpus=1 \\\n",
        "    --dropout=0.2 \\\n",
        "    --learning_rate=0.2 \\\n",
        "    --metrics=bleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Job id 0\r\n",
            "# hparams:\r\n",
            "  src=a\r\n",
            "  tgt=b\r\n",
            "  train_prefix=/content/nmt_model/train.bpe\r\n",
            "  dev_prefix=/content/nmt_model/test.bpe\r\n",
            "  test_prefix=/content/nmt_model/test.bpe\r\n",
            "  out_dir=/content/nmt_model\r\n",
            "# Vocab file /content/nmt_model/revocab.train.bpe.a exists\n",
            "The first 3 vocab words [.@@, ,, .] are not [<unk>, <s>, </s>]\n",
            "# Vocab file /content/nmt_model/revocab.train.bpe.b exists\n",
            "The first 3 vocab words [.@@, ., ,] are not [<unk>, <s>, </s>]\n",
            "  saving hparams to /content/nmt_model/hparams\n",
            "  saving hparams to /content/nmt_model/best_bleu/hparams\n",
            "  attention=\n",
            "  attention_architecture=standard\n",
            "  avg_ckpts=False\n",
            "  batch_size=16\n",
            "  beam_width=0\n",
            "  best_bleu=0\n",
            "  best_bleu_dir=/content/nmt_model/best_bleu\n",
            "  check_special_token=True\n",
            "  colocate_gradients_with_ops=True\n",
            "  decay_scheme=\n",
            "  dev_prefix=/content/nmt_model/test.bpe\n",
            "  dropout=0.2\n",
            "  embed_prefix=None\n",
            "  encoder_type=uni\n",
            "  eos=</s>\n",
            "  epoch_step=0\n",
            "  forget_bias=1.0\n",
            "  infer_batch_size=32\n",
            "  init_op=uniform\n",
            "  init_weight=0.1\n",
            "  learning_rate=0.2\n",
            "  length_penalty_weight=0.0\n",
            "  log_device_placement=False\n",
            "  max_gradient_norm=5.0\n",
            "  max_train=0\n",
            "  metrics=['bleu']\n",
            "  num_buckets=5\n",
            "  num_decoder_layers=2\n",
            "  num_decoder_residual_layers=0\n",
            "  num_embeddings_partitions=0\n",
            "  num_encoder_layers=2\n",
            "  num_encoder_residual_layers=0\n",
            "  num_gpus=1\n",
            "  num_inter_threads=0\n",
            "  num_intra_threads=0\n",
            "  num_keep_ckpts=5\n",
            "  num_layers=2\n",
            "  num_train_steps=45000000\n",
            "  num_translations_per_input=1\n",
            "  num_units=128\n",
            "  optimizer=sgd\n",
            "  out_dir=/content/nmt_model\n",
            "  output_attention=True\n",
            "  override_loaded_hparams=False\n",
            "  pass_hidden_state=True\n",
            "  random_seed=None\n",
            "  residual=False\n",
            "  sampling_temperature=0.0\n",
            "  share_vocab=False\n",
            "  sos=<s>\n",
            "  src=a\n",
            "  src_embed_file=\n",
            "  src_max_len=50\n",
            "  src_max_len_infer=None\n",
            "  src_vocab_file=/content/nmt_model/revocab.train.bpe.a\n",
            "  src_vocab_size=32605\n",
            "  steps_per_external_eval=None\n",
            "  steps_per_stats=100000\n",
            "  subword_option=\n",
            "  test_prefix=/content/nmt_model/test.bpe\n",
            "  tgt=b\n",
            "  tgt_embed_file=\n",
            "  tgt_max_len=50\n",
            "  tgt_max_len_infer=None\n",
            "  tgt_vocab_file=/content/nmt_model/revocab.train.bpe.b\n",
            "  tgt_vocab_size=32311\n",
            "  time_major=True\n",
            "  train_prefix=/content/nmt_model/train.bpe\n",
            "  unit_type=lstm\n",
            "  vocab_prefix=/content/nmt_model/revocab.train.bpe\n",
            "  warmup_scheme=t2t\n",
            "  warmup_steps=0\n",
            "# creating train graph ...\n",
            "  num_layers = 2, num_residual_layers=0\n",
            "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
            "  learning_rate=0.2, warmup_steps=0, warmup_scheme=t2t\n",
            "  decay_scheme=, start_decay_step=45000000, decay_steps 0, decay_factor 1\n",
            "# Trainable variables\n",
            "  embeddings/encoder/embedding_encoder:0, (32605, 128), /device:GPU:0\n",
            "  embeddings/decoder/embedding_decoder:0, (32311, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 32311), \n",
            "# creating eval graph ...\n",
            "  num_layers = 2, num_residual_layers=0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "# Trainable variables\n",
            "  embeddings/encoder/embedding_encoder:0, (32605, 128), /device:GPU:0\n",
            "  embeddings/decoder/embedding_decoder:0, (32311, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 32311), \n",
            "# creating infer graph ...\n",
            "  num_layers = 2, num_residual_layers=0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
            "# Trainable variables\n",
            "  embeddings/encoder/embedding_encoder:0, (32605, 128), /device:GPU:0\n",
            "  embeddings/decoder/embedding_decoder:0, (32311, 128), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
            "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 32311), \n",
            "# log_file=/content/nmt_model/log_1529870127\n",
            "2018-06-24 19:55:27.753926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-06-24 19:55:27.754461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2018-06-24 19:55:27.754502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\n",
            "2018-06-24 19:55:28.106090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-06-24 19:55:28.106146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \n",
            "2018-06-24 19:55:28.106174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \n",
            "2018-06-24 19:55:28.106588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10765 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2018-06-24 19:55:28.107415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\n",
            "2018-06-24 19:55:28.107486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-06-24 19:55:28.107514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \n",
            "2018-06-24 19:55:28.107535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \n",
            "2018-06-24 19:55:28.107809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10765 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2018-06-24 19:55:28.108053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\n",
            "2018-06-24 19:55:28.108100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-06-24 19:55:28.108122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \n",
            "2018-06-24 19:55:28.108140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \n",
            "2018-06-24 19:55:28.108493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10765 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "  created train model with fresh parameters, time 0.18s\n",
            "  created infer model with fresh parameters, time 0.08s\n",
            "  # 232\n",
            "    src: once in a while , stea@@ med du@@ mp@@ lin@@ gs . but only when she was drunk .\n",
            "    ref: that's rough , man . if i don't get st@@ ea@@ med du@@ mp@@ lin@@ gs once a week , i go crazy .\n",
            "    nmt: ao-@@ scots backbone joshua block@@ assyrian assyrian stupidity bulk spouting spouting 405 405 surren@@ yvonne set-up set-up set-up respirator presiden@@ presiden@@ sity sity sity sity moments grasshopper grasshopper thoughts thoughts thoughts thoughts city-d@@ board@@ board@@ aphrodisi@@ aphrodisi@@ assassinated\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  created eval model with fresh parameters, time 0.12s\n",
            "  eval dev: perplexity 32310.67, time 23s, Sun Jun 24 19:55:53 2018.\n",
            "  eval test: perplexity 32310.67, time 23s, Sun Jun 24 19:56:16 2018.\n",
            "2018-06-24 19:56:16.650384: I tensorflow/core/kernels/lookup_util.cc:373] Table trying to initialize from file /content/nmt_model/revocab.train.bpe.b is already initialized.\n",
            "2018-06-24 19:56:16.650764: I tensorflow/core/kernels/lookup_util.cc:373] Table trying to initialize from file /content/nmt_model/revocab.train.bpe.b is already initialized.\n",
            "2018-06-24 19:56:16.650971: I tensorflow/core/kernels/lookup_util.cc:373] Table trying to initialize from file /content/nmt_model/revocab.train.bpe.a is already initialized.\n",
            "  created infer model with fresh parameters, time 0.06s\n",
            "# Start step 0, lr 0.2, Sun Jun 24 19:56:16 2018\n",
            "# Init train iterator, skipping 0 elements\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5-4FB5pXjD4i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are several things to note here:\n",
        "\n",
        "num_train_steps — this is the amount of steps that the network will take before stopping, I make it big since it is always better to be in a situation when I need to stop training manually than in the situation when the network stopped when I have not expected it to stop;\n",
        "steps_per_stats — frequency in which network will output some stats. A thing to keep in mind here: outputting stats takes time so I need to find balance between outputting this too often and training model in a complete dark;\n",
        "metrics — logic of computing the distance between 2 sentences to evaluate model quality;\n",
        "Important things here is not to use “%%bash” but to use “!”. “%%bash” will wait till the cell completely executed before showing the output, which means almost forever due to the training of the model. On opposite, the “!” is showing output dynamically.\n",
        "\n",
        "As soon as training finishes its first epoch I can start chatting with the model.\n",
        "\n",
        "Let us be sure to kill the training if it is still in progress, otherwise I will not be able to chat with the model.\n",
        "\n",
        "Next, I copy paste following code in a file (lets say chat.sh) under nmt directory and run it like ./chat.sh <path to the model>. I might need to change the permission like chmod +x chat.sh."
      ]
    },
    {
      "metadata": {
        "id": "KPXoAJk9YUrW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pwd\n",
        "cd nmt\n",
        "touch /content/output\n",
        "chat () {\n",
        "   echo $1 > /content/input\n",
        "   python3 $HOME/pre_processing.py /content/input >/content/input.pre\n",
        "   $HOME/subword-nmt/apply_bpe.py -c $HOME/code.bpe --vocabulary $HOME/vocab.train.bpe.a --vocabulary-threshold 5 < /content/input.pre > /content/input.pre.bpe\n",
        "   cd $HOME/nmt\n",
        "   python -m nmt.nmt  --out_dir=/content/nmt_model --inference_input_file=/content/input.pre.bpe --inference_output_file=/content/output > /dev/null 2>&1\n",
        "   cat /content/output\n",
        "}\n",
        "chat \"hi\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}