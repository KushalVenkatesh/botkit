{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extractive Text Summarization using TextRank Algorithm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/KushalVenkatesh/botkit/blob/master/Extractive_Text_Summarization_using_TextRank_Algorithm.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "NxpLNN91IVHK",
        "colab_type": "code",
        "colab": {},
        "outputId": "e49f554b-06e1-4d9f-b228-72c4a998ca33"
      },
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/d/python2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XQWGZR5FIVHQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "45dbc714-a117-4602-f5f9-16a0dac01455"
      },
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 2.7.15 :: Anaconda, Inc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "tY_uNXGSIVHV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Extractive versus Abstractive\n",
        "\n",
        "The extractive approach entails selecting the X most representative sentences that best cover the whole information expressed by the original text. This is the most popular approach, especially because it’s a much easier task than the abstractive approach.\n",
        "\n",
        "In the abstractive approach, we basically build a summary of the text, in the way a human would build one. We pick ideas from several paragraphs or sentences, build readable sentences and present them in a concise form. "
      ]
    },
    {
      "metadata": {
        "id": "jSQU_QTzIVHW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The text summarization is mainly useful because it condenses information for easier consumption and analysis. "
      ]
    },
    {
      "metadata": {
        "id": "9XGWO5GSIVHW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The TextRank Algorithm\n",
        "Here is a very popular and accurate extractive text summarization algorithm. It’s called TextRank. Before diving into TextRank algorithm, we must first make sure we understand the PageRank algorithm, because it’s the foundation of TextRank."
      ]
    },
    {
      "metadata": {
        "id": "2-94XSfYIVHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We might have already heard of PageRank since it’s used heavily on the service we use most on the web: Google Search. It’s used to compute the rank of web pages, but funny enough, it’s not named after its use (ranking pages) but rather after its creator: Larry Page, one of Google’s founders. Obviously, the algorithm used inside Google is almost transfigured to be recognised as PageRank. This is due to the fact that inside Google the algorithm works at an incredible scale. We’ll be focusing here only on the original, plain PageRank algorithm. The algorithm itself is very interesting and worth our attention. \n",
        "\n",
        "Here are the fundamentals of it:\n",
        "\n",
        "The main insight: Important pages are linked by important pages (a recurrent definition)\n",
        "The PageRank value of a page is essentially the probability of a user visiting that page\n",
        "\n",
        "Let’s now define the variables we’ll be using:\n",
        "\n",
        "We have N pages with links between them\n",
        "P = PageRank vector (P[i] = the pagerank of page i)\n",
        "\n",
        "A[i][j] is the probability of the user transitioning from page i to page j\n",
        "A[i][j] is initialized with 1.0 / outbound_links_count(from_node=i) if there’s a link between i and j and 0 otherwise\n",
        "\n",
        "If a page i has no outbound links, then A[i][j] = 1.0/N. Assign equal probability to transitioning to any page. Pages with no outbound links are called dangling pages.\n",
        "\n",
        "Let’s write a basic implementation of PageRank. First, we’ll write a simple 10 web page graph to apply the algorithm on:"
      ]
    },
    {
      "metadata": {
        "id": "EOkynRekIVHY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "links = {\n",
        "    'webpage-1': set(['webpage-2', 'webpage-4', 'webpage-5', 'webpage-6', 'webpage-8', 'webpage-9', 'webpage-10']),\n",
        "    'webpage-2': set(['webpage-5', 'webpage-6']),\n",
        "    'webpage-3': set(['webpage-10']),\n",
        "    'webpage-4': set(['webpage-9']),\n",
        "    'webpage-5': set(['webpage-2', 'webpage-4']),\n",
        "    'webpage-6': set([]), # dangling page\n",
        "    'webpage-7': set(['webpage-1', 'webpage-3', 'webpage-4']),\n",
        "    'webpage-8': set(['webpage-1']),\n",
        "    'webpage-9': set(['webpage-1', 'webpage-2', 'webpage-3', 'webpage-8', 'webpage-10']),\n",
        "    'webpage-10': set(['webpage-2', 'webpage-3', 'webpage-8', 'webpage-9']),\n",
        "}\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzudngM8IVHb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will write a function that builds an index of the webpages and assigns them to a numeric index. We will use this to build an adjacency matrix."
      ]
    },
    {
      "metadata": {
        "id": "RMUVcIznIVHb",
        "colab_type": "code",
        "colab": {},
        "outputId": "2f6c9143-7489-41fe-e83a-8b8844bdf7b3"
      },
      "cell_type": "code",
      "source": [
        "def build_index(links):\n",
        "    website_list = links.keys()\n",
        "    return {website: index for index, website in enumerate(website_list)}\n",
        " \n",
        "website_index = build_index(links)\n",
        "print(website_index)\n",
        "# {'webpage-10': 3, 'webpage-9': 0, 'webpage-8': 1, 'webpage-1': 2, 'webpage-3': 4, 'webpage-2': 5, 'webpage-5': 6, 'webpage-4': 7, 'webpage-7': 8, 'webpage-6': 9}\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'webpage-10': 3, 'webpage-9': 0, 'webpage-8': 1, 'webpage-1': 2, 'webpage-3': 4, 'webpage-2': 5, 'webpage-5': 6, 'webpage-4': 7, 'webpage-7': 8, 'webpage-6': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JeGRum8KIVHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to feed a transition probability matrix to PageRank. A[i][j] is the probability of transitioning from page i to page j. Here’s how to build that:"
      ]
    },
    {
      "metadata": {
        "id": "TY-mwceyIVHe",
        "colab_type": "code",
        "colab": {},
        "outputId": "f2bb5feb-2521-40b7-d3df-c2fa8d31641f"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        " \n",
        "def build_transition_matrix(links, index):\n",
        "    total_links = 0\n",
        "    A = np.zeros((len(index), len(index)))\n",
        "    for webpage in links:\n",
        "        # dangling page\n",
        "        if not links[webpage]:\n",
        "            # Assign equal probabilities to transition to all the other pages\n",
        "            A[index[webpage]] = np.ones(len(index)) / len(index)\n",
        "        else:\n",
        "            for dest_webpage in links[webpage]:\n",
        "                total_links += 1\n",
        "                A[index[webpage]][index[dest_webpage]] = 1.0 / len(links[webpage])\n",
        " \n",
        "    return A\n",
        " \n",
        "A = build_transition_matrix(links, website_index)\n",
        "print(A)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.2        0.2        0.2        0.2        0.2\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.14285714 0.14285714 0.         0.14285714 0.         0.14285714\n",
            "  0.14285714 0.14285714 0.         0.14285714]\n",
            " [0.25       0.25       0.         0.         0.25       0.25\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.5        0.         0.         0.5       ]\n",
            " [0.         0.         0.         0.         0.         0.5\n",
            "  0.         0.5        0.         0.        ]\n",
            " [1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.33333333 0.         0.33333333 0.\n",
            "  0.         0.33333333 0.         0.        ]\n",
            " [0.1        0.1        0.1        0.1        0.1        0.1\n",
            "  0.1        0.1        0.1        0.1       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oVpnXRBJIVHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we’ll write the actual PageRank algorithm. The algorithm takes 2 parameters:\n",
        "1. eps: stop the algorithm when the difference between 2 consecutive iterations is smaller or equal to eps\n",
        "2. d: damping factor: With a probability of 1-d the user will simply pick a web page at random as the next destination, ignoring the link structure completely."
      ]
    },
    {
      "metadata": {
        "id": "DtjCdamfIVHh",
        "colab_type": "code",
        "colab": {},
        "outputId": "61364dc9-9fd7-4b96-bf59-d1af91813bca"
      },
      "cell_type": "code",
      "source": [
        "def pagerank(A, eps=0.0001, d=0.85):\n",
        "    P = np.ones(len(A)) / len(A)\n",
        "    while True:\n",
        "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
        "        delta = abs(new_P - P).sum()\n",
        "        if delta <= eps:\n",
        "            return new_P\n",
        "        P = new_P\n",
        " \n",
        "results = pagerank(A)\n",
        " \n",
        "print(\"Results:\", results) # [ 0.13933698,  0.09044235,  0.1300934 ,  0.13148714,  0.08116465, 0.1305122 ,  0.09427366,  0.085402  ,  0.02301397,  0.09427366]\n",
        "print(sum(results)) # 1.0\n",
        "print([item[0] for item in sorted(enumerate(results), key=lambda item: -item[1])]) # [0, 3, 5, 2, 6, 9, 1, 7, 4, 8]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Results:', array([0.13933698, 0.09044235, 0.1300934 , 0.13148714, 0.08116465,\n",
            "       0.1305122 , 0.09427366, 0.085402  , 0.02301397, 0.09427366]))\n",
            "1.0000000000000002\n",
            "[0, 3, 5, 2, 6, 9, 1, 7, 4, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zAaAJJwgIVHk",
        "colab_type": "code",
        "colab": {},
        "outputId": "65493307-eb69-4142-ce2b-ca435c6bfa7a"
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk downloader"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in d:\\python2\\lib\\site-packages (3.3)\n",
            "Collecting downloader\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/ee/78bff07448dbf9fd03e45f3ec40ab09a4e0f1827508124b98f3c83d49af4/downloader-0.98.tar.gz\n",
            "Requirement already satisfied: six in d:\\python2\\lib\\site-packages (from nltk) (1.11.0)\n",
            "Building wheels for collected packages: downloader\n",
            "  Running setup.py bdist_wheel for downloader: started\n",
            "  Running setup.py bdist_wheel for downloader: finished with status 'done'\n",
            "  Stored in directory: C:\\Users\\kusha\\AppData\\Local\\pip\\Cache\\wheels\\b2\\eb\\46\\10c34ef0b1d8fad0f28c2514e44a69502e58dc47debe15f419\n",
            "Successfully built downloader\n",
            "Installing collected packages: downloader\n",
            "Successfully installed downloader-0.98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "distributed 1.21.8 requires msgpack, which is not installed.\n",
            "grin 1.2.1 requires argparse>=1.1, which is not installed.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fa0Y91U2IVHo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From PageRank to TextRank:\n",
        "\n",
        "We might be asking ourself: “How does PageRank help me with text summarization?”\n",
        "\n",
        "Here’s the trick:\n",
        "\n",
        "We consider sentences the equivalent of web pages\n",
        "The probability of going from sentence A to sentence B is equal to the similarity of the 2 sentences\n",
        "Apply the PageRank algorithm over this sentence graph\n",
        "\n",
        "A few observations:\n",
        "\n",
        "In the case of TextRank, the graph is symmetrical\n",
        "It’s up to us to come up with a similarity measure and the algorithm’s performance depends strongly upon this measure\n",
        "The algorithm is language agnostic. (It can become language dependent due to a language dependent implementation of the similarity measure)\n",
        "It doesn’t require any training, it’s a totally unsupervised method (which we like)\n",
        "Here are some conditions a good similarity measure has to obey:\n",
        "\n",
        "0 <= similarity(A, B) <= 1\n",
        "similarity(A, A) = 1\n",
        "similarity(A, B) =/= 1 if A =/= B\n",
        "\n",
        "A widely used measure in Natural Language Processing is the Cosine Similarity. The Cosine Similarity computes the cosine of the angle between 2 vectors. If the vectors are identical, the cosine is 1.0. If the vectors are orthogonal, the cosine is 0.0. This means the cosine similarity is a measure we can use.\n",
        "\n",
        "NLTK implements cosine_distance, which is 1 - cosine_similarity. The concept of distance is opposite to similarity. Two identical vectors are located at 0 distance and are 100% similar.\n",
        "\n",
        "Let’s write a good sentence similarity measure:"
      ]
    },
    {
      "metadata": {
        "id": "RpFtN7tQIVHo",
        "colab_type": "code",
        "colab": {},
        "outputId": "877c1d4d-9528-486f-ea66-4248e1e05b45"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        " \n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "# One out of 5 words differ => 0.8 similarity\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))\n",
        " \n",
        "# One out of 2 non-stop words differ => 0.5 similarity\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split(), stopwords.words('english')))\n",
        " \n",
        "# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a good sentence\".split(), stopwords.words('english')))\n",
        " \n",
        "# Completely different sentences=> 0.0\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"I want to go to the market\".split(), stopwords.words('english')))\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "0.7999999999999998\n",
            "0.4999999999999999\n",
            "0.9999999999999998\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uy3AqHekIVHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here let’s build the PageRank transition matrix by building the sentence similarity matrix:"
      ]
    },
    {
      "metadata": {
        "id": "LynGK5UxIVHs",
        "colab_type": "code",
        "colab": {},
        "outputId": "c5cf8969-d50c-4d75-f1c4-5c5d6ffcfa55"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "#nltk.download()\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        " \n",
        "# Get a text from the Brown Corpus\n",
        "sentences = brown.sents('ca01')\n",
        " \n",
        "print(sentences)\n",
        "# [[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.'], [u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.'], ...]\n",
        " \n",
        "print(len(sentences))  #  98\n",
        " \n",
        "# get the english list of stopwords\n",
        "stop_words = stopwords.words('english')\n",
        " \n",
        " \n",
        "def build_similarity_matrix(sentences, stopwords=None):\n",
        "    # Create an empty similarity matrix\n",
        "    S = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2:\n",
        "                continue\n",
        " \n",
        "            S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        " \n",
        "    # normalize the matrix row-wise\n",
        "    for idx in range(len(S)):\n",
        "        S[idx] /= S[idx].sum()\n",
        " \n",
        "    return S\n",
        " \n",
        "S = build_similarity_matrix(sentences, stop_words)    \n",
        "print(S)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\kusha\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\brown.zip.\n",
            "[[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.'], [u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.'], ...]\n",
            "98\n",
            "[[0.         0.02171602 0.02438459 ... 0.01056602 0.02113203 0.0224139 ]\n",
            " [0.01642812 0.         0.00853223 ... 0.00646989 0.01940966 0.0137247 ]\n",
            " [0.0326456  0.01509956 0.         ... 0.00642841 0.01928522 0.02727342]\n",
            " ...\n",
            " [0.0154814  0.01253106 0.00703547 ... 0.         0.03200945 0.0150894 ]\n",
            " [0.01453939 0.01765287 0.00991107 ... 0.01503088 0.         0.02125687]\n",
            " [0.0179617  0.01453869 0.01632527 ... 0.00825283 0.02475849 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MSTtpeQIIVH0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we know how to compute the similarity matrix, we can compute the summary. For demonstration purposes, we’re using the first document from the Brown corpus. Here’s a rough version of the code:"
      ]
    },
    {
      "metadata": {
        "id": "6Wckeu0LIVH1",
        "colab_type": "code",
        "colab": {},
        "outputId": "10c10efb-526c-4dcf-9f2d-3893da148e13"
      },
      "cell_type": "code",
      "source": [
        "from operator import itemgetter \n",
        " \n",
        "sentence_ranks = pagerank(S)\n",
        " \n",
        "print(sentence_ranks)\n",
        " \n",
        "# Get the sentences ordered by rank\n",
        "ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
        "print(ranked_sentence_indexes)\n",
        " \n",
        "# Suppose we want the 5 most import sentences\n",
        "SUMMARY_SIZE = 5\n",
        "SELECTED_SENTENCES = sorted(ranked_sentence_indexes[:SUMMARY_SIZE])\n",
        "print(SELECTED_SENTENCES)\n",
        " \n",
        "# Fetch the most important sentences\n",
        "summary = itemgetter(*SELECTED_SENTENCES)(sentences)\n",
        " \n",
        "# Print the actual summary\n",
        "for sentence in summary:\n",
        "    print(' '.join(sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01121618 0.01416586 0.00875955 0.01720666 0.01097157 0.00992082\n",
            " 0.01200115 0.00168689 0.01309704 0.01454477 0.01097302 0.00816622\n",
            " 0.00878952 0.00681878 0.01487246 0.00980958 0.01631003 0.01077275\n",
            " 0.01016464 0.00164935 0.01201909 0.01345858 0.01054308 0.01246479\n",
            " 0.00189483 0.00848166 0.01189495 0.00226414 0.00871032 0.01280831\n",
            " 0.01201529 0.0091875  0.0132896  0.01413383 0.0083716  0.01075297\n",
            " 0.01131163 0.00866148 0.00781889 0.00798044 0.01358306 0.00816582\n",
            " 0.00787996 0.00986716 0.01055781 0.01245516 0.00559471 0.00901723\n",
            " 0.01349356 0.01029195 0.00615882 0.00646616 0.00680286 0.01227652\n",
            " 0.00915185 0.01146637 0.01031809 0.00676416 0.00947115 0.00730678\n",
            " 0.00687184 0.0022958  0.01043746 0.00837449 0.00795457 0.00182517\n",
            " 0.0082859  0.00822529 0.01303836 0.00667884 0.00853634 0.01013017\n",
            " 0.00852789 0.01224895 0.00630303 0.01017482 0.01098564 0.01494719\n",
            " 0.00789689 0.01391384 0.01242932 0.0017009  0.01639154 0.01258241\n",
            " 0.01145775 0.01336677 0.01761915 0.01135941 0.01403542 0.01388494\n",
            " 0.01070144 0.01214065 0.01511402 0.01502419 0.01015592 0.00820689\n",
            " 0.01549813 0.0136297 ]\n",
            "[86, 3, 82, 16, 96, 92, 93, 77, 14, 9, 1, 33, 88, 79, 89, 97, 40, 48, 21, 85, 32, 8, 68, 29, 83, 23, 45, 80, 53, 73, 91, 20, 30, 6, 26, 55, 84, 87, 36, 0, 76, 10, 4, 17, 35, 90, 44, 22, 62, 56, 49, 75, 18, 94, 71, 5, 43, 15, 58, 31, 54, 47, 12, 2, 28, 37, 70, 72, 25, 63, 34, 66, 67, 95, 11, 41, 39, 64, 78, 42, 38, 59, 60, 13, 52, 57, 69, 51, 74, 50, 46, 61, 27, 24, 65, 81, 7, 19]\n",
            "[3, 16, 82, 86, 96]\n",
            "`` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
            "Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said .\n",
            "-- After a long , hot controversy , Miller County has a new school superintendent , elected , as a policeman put it , in the `` coolest election I ever saw in this county '' .\n",
            "`` This was the coolest , calmest election I ever saw '' , Colquitt Policeman Tom Williams said .\n",
            "`` Everything went real smooth '' , the sheriff said .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bToTIVKAIVH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Let’s bundle all this code in a nice & neat function:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S8P1A90jIVH6",
        "colab_type": "code",
        "colab": {},
        "outputId": "44862bbf-7e54-44f9-b912-becbd4ca35ee"
      },
      "cell_type": "code",
      "source": [
        "def textrank(sentences, top_n=5, stopwords=None):\n",
        "    \"\"\"\n",
        "    sentences = a list of sentences [[w11, w12, ...], [w21, w22, ...], ...]\n",
        "    top_n = how may sentences the summary should contain\n",
        "    stopwords = a list of stopwords\n",
        "    \"\"\"\n",
        "    S = build_similarity_matrix(sentences, stop_words) \n",
        "    sentence_ranks = pagerank(S)\n",
        " \n",
        "    # Sort the sentence ranks\n",
        "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
        "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
        "    summary = itemgetter(*SELECTED_SENTENCES)(sentences)\n",
        "    return summary\n",
        " \n",
        "for idx, sentence in enumerate(textrank(sentences, stopwords=stopwords.words('english'))):\n",
        "    print(\"%s. %s\" % ((idx + 1), ' '.join(sentence)))\n",
        " \n",
        "# 1. `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
        "# 2. Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said .\n",
        "# 3. -- After a long , hot controversy , Miller County has a new school superintendent , elected , as a policeman put it , in the `` coolest election I ever saw in this county '' .\n",
        "# 4. `` This was the coolest , calmest election I ever saw '' , Colquitt Policeman Tom Williams said .\n",
        "# 5. `` Everything went real smooth '' , the sheriff said .\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' .\n",
            "2. Nevertheless , `` we feel that in the future Fulton County should receive some portion of these available funds '' , the jurors said .\n",
            "3. -- After a long , hot controversy , Miller County has a new school superintendent , elected , as a policeman put it , in the `` coolest election I ever saw in this county '' .\n",
            "4. `` This was the coolest , calmest election I ever saw '' , Colquitt Policeman Tom Williams said .\n",
            "5. `` Everything went real smooth '' , the sheriff said .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TAXU3ObyIVH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conclusions:\n",
        "\n",
        "1.TextRank uses the intuition behind the PageRank algorithm to rank sentences\n",
        "\n",
        "2.TextRank is an unsupervised method for computing the extractive summary of a text. No training is necessary.\n",
        "\n",
        "3.Given that it’s language independent, it an be used on any language, not only English.\n",
        "\n",
        "4.In the above exercise, we have used the cosine similarity as the similarity measure. We have also converted sentences to vectors to compute this similarity."
      ]
    }
  ]
}
